{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e4fc5f-fae3-4bb6-bc6f-cba7fe8ef31f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance,var_samp,var_pop\n",
    "from pyspark.sql import SparkSession\n",
    "Spark=SparkSession.builder.appName(\"bojappapractice\").getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae2e3ad1-d867-4d73-a172-cf6207d8b7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/FileStore/Mahesh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a423d3a5-e888-4042-8b65-3e2506a23abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.Reading a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51b2b18f-224f-4c44-97ba-d481282646be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"InferSchema\",\"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .load(\"/FileStore/Mahesh/accepted_2007_to_2018Q4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da3b4f5e-4b2f-4d40-9e0b-1ce3c57d3284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1faca6c4-e8c4-40b5-9712-aeb9edead0c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2.Read all CSV Files under a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c635a75-3c36-4ba7-bdc0-15dfa7f2f160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"inferschema\",True).option(\"sep\",\",\").load(\"/FileStore/boja1.csv\")\n",
    "df.show(truncate=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e24f348f-90a6-4585-9030-78dba0a385b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##07. Databricks | Pyspark: Filter Condition\n",
    "#### 1.Single&Multiple conditions\n",
    "####2.Starts with\n",
    "####3.Ends with\n",
    "####4.Contains\n",
    "####5.Like\n",
    "####6.Null values\n",
    "####7.Is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf4b47c-3992-46d7-b96a-a38b9bdf8c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a dataframe\n",
    "\n",
    "\n",
    "employee_data = [(10, \"Raj Kumar\", \"1999\", \"100\", \"Μ\", 2000), (20, \"Rahul Rajan\", \"2002\", \"200\", \"F\", 8000), (30, \"Raghav\", \"2010\", \"100\", None, 6000), (40, \"Raja Singh\", \"2004\", \"100\", \"F\", 7000), (50, \"Rama Krish\" , \"2008\", \"400\", \"Μ\", 1000), (60, \"Rasul\", \"2014\", \"500\", \"Μ\", 5000), (70, \"Kumar Chand\", \"2004\", \"600\", \"Μ\", 5000)]\n",
    "\n",
    "employee_schema = [\"employee_id\", \"name\",\"doj\", \"employee_dept_id\", \"gender\", \"salary\"]\n",
    "\n",
    "employee_df = spark.createDataFrame(data=employee_data, schema=employee_schema)\n",
    "\n",
    "display (employee_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bace08aa-8756-45fc-af59-d5b6a7d459cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display (employee_df)\n",
    "display(employee_df.filter(employee_df.salary==5000))  # >,< =,=>,=<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b9bb3f-ef7c-4ed4-8b39-07074bcefb7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filterdf=employee_df.filter(employee_df.salary!=5000)\n",
    "filterdf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "254324e7-33b0-4d97-9cf4-d3e504b402f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using AND Condition\n",
    "employee_df.display()\n",
    "multifilter_df=employee_df.filter((employee_df.gender=='F')&(employee_df.doj=='2004'))\n",
    "multifilter_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a553b36-8f01-48b2-8209-59fff97ccf87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(employee_df.select(\"name\",\"gender\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9a8240-8460-4fb4-a52a-44dfa22abbf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using OR Condition\n",
    "employee_df.display()\n",
    "multifilter_df=employee_df.filter((employee_df.gender=='F')|(employee_df.doj=='2004'))\n",
    "multifilter_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871269d3-7107-4506-8091-b7e15001459b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(employee_df)\n",
    "display(employee_df.filter(employee_df.name.endswith('h')))  #startswith,endswith,contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d34e497-eaef-4fcb-ba2f-58dd2873b6f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(employee_df)\n",
    "display(employee_df.filter(employee_df.name.startswith('Ku')))  #startswith,endswith,contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd39f95-0110-4730-a757-aec05a0de607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(employee_df)\n",
    "display(employee_df.filter(employee_df.name.contains('Ku')))  #startswith,endswith,contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e161eeb-3f1b-4ec0-a28a-218c32de7cf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(employee_df)\n",
    "display(employee_df.filter(employee_df.gender.isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb57299d-27b7-429c-a7ef-84b6d83916a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(employee_df)\n",
    "display(employee_df.filter(employee_df.gender.isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4ce1cd-b058-4cc1-a29a-bdbef23ac173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(employee_df)\n",
    "display(employee_df.filter(employee_df.employee_id.isin(20,70)))   #isin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a436256e-5dfe-427f-9729-8b052e953b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(employee_df)\n",
    "display(employee_df.filter(employee_df.name.like('%Kumar')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b316c8e-7edd-4e9c-9377-c798bca55d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####08. Databricks | Pyspark: Add, Rename and Drop Columns\n",
    "#####Add columns\n",
    "#####df.withColumn(\"nuew_column_name\",value)\n",
    "##===========================================================\n",
    "#####Rename column\n",
    "####df.withColumnRenamed(\"old_name\",\"new_name\")\n",
    "####Drop Column:\n",
    "###df.drop(\"Column_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02add838-6939-4d63-9b09-58745d5b7acb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "employee_data = [(16, \"Raj\", \"Kumar\", \"1999\",\"100\", \"Μ\", 2000), (20, \"Rahul\", \"Rajan\", \"2002\",\"200\",\"F\", 8000), (30, \"Raghav\", \"Manish\", \"2010\",\"100\", None, 6000), (40, \"Raja\", \"Singh\", \"2004\", \"100\", \"F\", 7000), (50, \"Rama\", \"Krish\", \"2008\", \"400\", \"Μ\", 1000), (60, \"Rasul\", \"Kutty\", \"2014\", \"500\",\"M\", 5000), (70, \"Kumar\", \"Chand\", \"2004\",\"600\",\"M\", 5000)]\n",
    "\n",
    "employee_schema = [\"employee_id\", \"first_name\", \"last_name\", \"doj\", \"employee_dept_id\", \"gender\", \"salary\"]\n",
    "employee_df = spark.createDataFrame (data=employee_data, schema=employee_schema)\n",
    "\n",
    "display(employee_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0e2fcd2-fc2c-48f0-8bbf-9116ecc5f57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add a new column using constant literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f87ba3c-ad15-4366-99f2-32ba83731515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "add_column_df=employee_df.withColumn(\"location\",lit(\"Mumbai\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63aac369-2f44-4cdf-bc48-3b6a24f537b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add a new column by Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04035ad-30e8-41d8-a345-3f1e69449720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat,col\n",
    "emp_df_add=employee_df.withColumn(\"Bonus\",employee_df.salary*2).withColumn(\"Fullname\",concat(\"first_name\",\"last_name\"))\n",
    "emp_df_add.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ccaa2e1-0dd5-4ac3-b1ce-40806c69dc5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#if you want space between firstname and last name\n",
    "from pyspark.sql.functions import concat,col\n",
    "emp_df_add=employee_df.withColumn(\"Bonus\",employee_df.salary*2).withColumn(\"Fullname\",concat(\"first_name\",lit(\" \"),\"last_name\"))\n",
    "emp_df_add.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d1cdd7d-8b36-4ffa-8f85-bef478719f4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Rename a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452b86a9-56e0-4fda-8c21-6f8d6b52e5e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df_rename=emp_df_add.withColumnRenamed(\"Fullname\",\"Full_Name\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a65732-e272-4541-b72d-1611bc91d5dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df_rename=emp_df_add.withColumnRenamed(\"Fullname\",\"Full_Name\").withColumnRenamed(\"doj\",\"Date_of_Joine\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f579476-044e-4fe5-9285-1e70a036d2ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "drop_df=emp_df_add.drop(\"Fullname\").show()    #.drop(\"doj\") for multiple columns drop just add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ead2459a-f0d8-445b-ba2d-0a91674b5d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##09. Databricks | PySpark Join Types\n",
    "\n",
    "####  DF1.join(DF2,DF1.key=DF2.key,\"join_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8151787-8b23-46ac-8b82-20a257652d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_data = [(10, \"Raj\",\"1999\",\"100\",\"M\",2000), (20, \"Rahul\", \"2002\",\"200\",\"Μ\",8000), (30, \"Raghav\",\"2010\",\"100\",\"\",6000), (40, \"Raja\", \"2004\",\"100\",\"F\",7000), (50, \"Rama\", \"2008\",\"400\",\"F\",1000), (60, \"Rasul\", \"2014\",\"500\", \"M\", 5000)]\n",
    "\n",
    "\n",
    "employee_schema = [\"employee_id\", \"name\", \"doj\", \"employee_dept_id\", \"gender\", \"salary\"]\n",
    "\n",
    "employeeDF=spark.createDataFrame(data=employee_data, schema=employee_schema)\n",
    "\n",
    "\n",
    "\n",
    "display(employeeDF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e2056e6-0c0b-4113-b839-9b0e978da126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "department_data=[(\"HR\",100), (\"Supply\", 200), (\"Sales\",300), (\"Stock\", 400)]\n",
    "\n",
    "department_schema =[\"dept name\",\"dept_id\"]\n",
    "\n",
    "departmentDF=spark.createDataFrame(data=department_data, schema=department_schema)\n",
    "display(departmentDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9adcd11-339e-4113-a0df-be9ad56cb04c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_join=employeeDF.join(departmentDF,employeeDF.employee_dept_id==departmentDF.dept_id,\"inner\")\n",
    "display(df_join)\n",
    "df_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3696e3f-01ea-4269-b493-c66cb0051ad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_join=employeeDF.join(departmentDF,employeeDF.employee_dept_id==departmentDF.dept_id,\"outer\")\n",
    "display(df_join)\n",
    "df_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c17ee2-829b-4006-b416-237b3d7c8f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_join=employeeDF.join(departmentDF,employeeDF.employee_dept_id==departmentDF.dept_id,\"left\")  #left_outer\n",
    "display(df_join)\n",
    "df_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94e231f-8c79-4f32-8c02-a314e4909f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_join=employeeDF.join(departmentDF,employeeDF.employee_dept_id==departmentDF.dept_id,\"right\") #right_outer\n",
    "display(df_join)\n",
    "df_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e78c81f2-5c3d-4174-826a-59827dcd2e19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_join=employeeDF.join(departmentDF,employeeDF.employee_dept_id==departmentDF.dept_id,\"left_semi\")\n",
    "display(df_join)\n",
    "df_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "073d2a1a-331e-423e-b784-8c3d2495d980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_join=employeeDF.join(departmentDF,employeeDF.employee_dept_id==departmentDF.dept_id,\"left_anti\")\n",
    "display(df_join)\n",
    "df_join.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb309b46-1d19-4caf-8c82-4534493b70e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##10. Databricks | Pyspark: Utility Commands - DBUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72867772-c751-460f-9efb-ecce665ba6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a54ef5a5-216e-47bb-a30c-f551233c27d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01fa419a-dea6-4c91-bd37-da3792cbaeb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##dbutils.notebook.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed9ef97-4b95-4d12-b604-0f5dc12e1177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb180ee6-24be-49af-8d7b-169619ed1137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##dbutils.widgets.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ef3f5a1-330f-4f7b-9b52-9216a3dc1228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.widgets.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bc2cacb-bcb5-434d-a700-f0bc7899f25f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "395b3b3c-380b-4e1f-aa46-215ac3953b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.help(\"get\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fffdf1d1-8905-4a90-bf5e-50193c142bbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##dbutils.fs.help(\"cp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b12def17-0f38-413a-a570-919da496f93a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.help(\"cp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17f8703d-58d9-47ef-ad73-c71b5c8e4cd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##dbutils.notebook.help(\"exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac24108-1247-4a42-8293-d160b4b79385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.help(\"exit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbac4d0d-72cb-4e1e-8b1f-a001d0dfdb11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DBUtils file systems commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f925eb4-cb94-45cc-a24d-05845555888e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/FileStore/Mahesh/overwrite_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8b04bf0-3a75-4258-9014-8681d443ea5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/Mahesh/rajade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40ee45ea-d3df-4ee5-92b0-7a5f7518c33a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"dbfs:/FileStore/shared_uploads/jbojappa@gmail.com/EMPLOYEE-1.csv\",\"dbfs:/FileStore/Mahesh/rajade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1524af-d393-4378-8789-4c3266ac5430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/FileStore/shared_uploads/jbojappa@gmail.com/EMPLOYEE-1.csv\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4b91dc5-9202-4af9-a75d-f98c4858e520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f974d54e-e983-4eae-8792-f216d4f32ae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"Folder_name\",\"\",\"\")\n",
    "dbutils.widgets.text(\"File_name\",\"\",\"\")\n",
    "#dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27b07ea-694f-4d49-8e63-f4aba8995295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Folder_location=dbutils.widgets.get(\"Folder_name\")\n",
    "File_location=dbutils.widgets.get(\"File_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e624f4ec-c360-440a-bcc1-60733c6bcc12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"Drop_down\",\"1\",[str(x) for x in range(1, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9154c5ac-2b9f-40b5-98e1-5eb376b70e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.combobox(\"combo_box\",\"1\",[str(x) for x in range(1, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9f3bc9-2162-47d9-a98f-d99add45797d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.multiselect(\"Product\",\"camera\",(\"camera\",\"gps\",\"smartphone\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5544d96c-bff9-4c94-99f6-6e7bba6008ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e863a3f-a863-4d7c-b3b0-28154a7f9cc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 11. Databricks | Pyspark: Explode Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7854a536-68e8-44bf-836f-cde70ba01f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "array_appliance = [ ('Raja', ['TV','Refrigerator','Oven','AC']), ('Raghav', ['AC', 'Washing machine', None]), ('Ram', ['Grinder','TV']), ('Ramesh', ['Refrigerator', 'TV', None]), ('Rajesh', None)]\n",
    "\n",
    "df_app = spark.createDataFrame(data=array_appliance, schema = ['name', 'Appliances'])\n",
    "\n",
    "df_app.printSchema()\n",
    "\n",
    "display(df_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72521806-7656-4383-8d58-4a6e901c8705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create a dataframe with map columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17903890-c71a-4e96-9512-a75390fc7b00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "map_brand = [\n",
    "('Raja', {'TV': 'LG', 'Refrigerator': 'Samsung', 'Oven': 'Philipps', 'AC': 'Voltas'}),\n",
    "('Raghav', {'AC': 'Samsung', 'Washing machine': 'LG'}),\n",
    "('Ram',{'Grinder': 'Preethi', 'TV':''}),\n",
    "('Ramesh', {'Refrigerator': 'LG', 'TV': 'Croma'}), \n",
    "('Rajesh', None)]\n",
    "\n",
    "df_brand = spark.createDataFrame(data=map_brand, schema = ['name', 'Brand'])\n",
    "\n",
    "df_brand.printSchema()\n",
    "display(df_brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05e09377-60cd-44f6-aaa4-36505f4008c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explode eliminate the null values it will not consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dce2b6ee-28e0-44d6-86ba-1edb3ec973f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df2=df_app.select(df_app.name,explode(df_app.Appliances))\n",
    "\n",
    "df_brand.printSchema()\n",
    "display(df_app)\n",
    "\n",
    "df2.printSchema()\n",
    "display(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4bb7a87-18ba-4e60-8b9e-91b94650106b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df3=df_brand.select(df_brand.name,explode(df_brand.Brand))\n",
    "df_brand.printSchema()\n",
    "display(df_brand)\n",
    "\n",
    "df3.printSchema()\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4caac1f4-5871-4741-929a-5c2b31006249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explode outer to consider Null VALUES\n",
    "#### It consider nullvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dba00fe-2875-499d-acba-eb48fd3c9f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "display(df_app.select(df_app.name,explode_outer(df_app.Appliances)))\n",
    "display(df_brand.select(df_brand.name,explode_outer(df_brand.Brand)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c376bfc6-d9fe-4d18-9de9-52029bcb0f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Positionall Explode\n",
    "## It gives the extra columna as pos position of values or key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccfc8dd3-8dea-4b95-9f54-793846ce9875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import posexplode\n",
    "display(df_app.select(df_app.name,posexplode(df_app.Appliances)))\n",
    "display(df_brand.select(df_brand.name,posexplode(df_brand.Brand)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aff5b74-4ea3-45d1-8421-01dee718b82b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Positional explode with null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6463efee-f81e-4f54-b546-7359f2444f10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import posexplode_outer\n",
    "display(df_app.select(df_app.name,posexplode_outer(df_app.Appliances)))\n",
    "display(df_brand.select(df_brand.name,posexplode_outer(df_brand.Brand)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "853a8b96-331b-4446-ae01-e46162859954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#12. Databricks | Pyspark: Case Function (When.Otherwise )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "056e9e0f-6140-43f6-a9fe-7f8603a0e102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_student = [(\"Raja\", \"Science\", 89, \"P\",90), \n",
    "                (\"Rakesh\", \"Maths\",90, \"P\", 70), \n",
    "                 (\"Rama\", \"English\",20,\"F\",80), \n",
    "                 (\"Ramesh\", \"Science\", 45, \"F\", 75),\n",
    "                  (\"Rajesh\", \"Maths\", 30, \"F\",50),\n",
    "                   (\"Raghav\", \"Maths\", None, \"NA\", 70)]\n",
    "\n",
    "Schema = [\"name\", \"Subject\", \"Mark\", \"Status\", \"Attendance\"]\n",
    "\n",
    "df = spark.createDataFrame(data = data_student, schema=Schema)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca9897d9-527b-4afe-9952-f812e79d255e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Update the existing column\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "df=df.withColumn(\"status\",when(df.Mark>=50,\"Pass\")\n",
    "                 .when(df.Mark<50,\"Fail\")\n",
    "                 .otherwise(\"Absentee\"))\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a33d47-275e-4e28-b9a1-705b54a407a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a new column\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df3=df.withColumn(\"New_status\",when(df.Mark>50,\"Pass\")\n",
    "                 .when(df.Mark<50,\"Fail\")\n",
    "                 .otherwise(\"Absentee\"))\n",
    "\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47bd4450-74fd-418b-affb-1b004260773b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Another syntax method\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df4=df.withColumn(\"New_status\", expr(\"CASE WHEN Mark >=50 THEN 'Pass' \" + \n",
    "                                     \"WHEN Mark <50 THEN 'Fail'\"+\n",
    "                                     \"ELSE 'Absentee' END\" ))\n",
    "display(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb31971-7efd-4d49-a565-a46a25657bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " #Multi conditions using AND  and OR Operators\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df6=df.withColumn(\"Grade\",when((df.Mark>=80)&(df.Attendance>=80),\"Distinct\").when((df.Mark>=50)&(df.Attendance>=50),\"Good\").otherwiese(\"Average\")\n",
    "                \n",
    "display(df6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1648e9ff-aa98-4ab3-96ae-c0598bdd1ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##\n",
    "13. Databricks | Pyspark: Union & UnionAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec77b84a-30ab-41f9-ae3b-19083eb5870f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.master(\"local\").getOrCreate()\n",
    "print(spark.sparkContext.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0cebaaf-33ef-4300-97b7-4798d2e32e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a dataframe\n",
    "employee_data = [(100, \"Stephen\", \"1999\",\"100\",\"M\", 2000), (200, \"Philipp\", \"2002\",\"200\",\"M\",8000), (300, \"John\", \"2010\",\"100\",\"\",6000)]\n",
    "\n",
    "employee_schema = [\"employee_id\", \"name\", \"doj\", \"employee_dept_id\",\"gender\", \"salary\"]\n",
    "\n",
    "df1= spark.createDataFrame(data=employee_data, schema = employee_schema)\n",
    "\n",
    "display(df1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f583c5-d6cd-43d4-ac97-3449572ce3ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create dataframe2\n",
    "employee_data = [ (300, \"John\", \"2010\",\"100\",\"\",6000), (400, \"Nancy\",\"2008\",\"400\",\"F\",1000), (500, \"Rosy\", \"2014\",\"500\",\"M\",5000)]\n",
    "\n",
    "empployee_schema = [\"employee_id\", \"name\",\"doj\", \"employee_dept_id\",\"gender\", \"salary\"]\n",
    "\n",
    "df2= spark.createDataFrame(data=employee_data, schema = employee_schema)\n",
    "\n",
    "display (df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12b401a2-9eca-481e-976c-7af6fdc8cf50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef63580-4c79-4e75-8672-22b2e178b2d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_union=df1.union(df2)\n",
    "display(df_union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a23c7b2f-6831-4a06-8a0d-2fd273c2a1ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Droup duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f389abfe-975d-4a73-9a6b-dacccc7c7de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_union.dropDuplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b525946-6742-4ef7-8a92-e7d188448b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Unionall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78eec4fd-37b3-4b2a-999b-97e3f251f70b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_union_all=df1.unionAll(df2)\n",
    "display(df_union_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30884b0e-2e22-477d-9978-d24eb3bbc3a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Schema Mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd31aba2-3fd2-44b3-a421-c820a8fd7a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Union can perform only same number of columns is matched from boath dataframes otherwiese error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e38765d-4784-45cd-9007-10cb86f512d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##14. Databricks | Pyspark: Pivot & Unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2466c04f-25c1-4ec8-b2a0-a89ef1333305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a sample dataframe\n",
    "\n",
    "data =[(\"ABC\", \"Q1\", 2000),\n",
    "   (\"ABC\", \"Q2\", 3000),\n",
    "   (\"ABC\", \"Q3\", 6000),\n",
    "   (\"ABC\", \"Q4\", 1000),\n",
    "    (\"ΧLM\", \"Q1\", 5000),\n",
    "    (\"XYZ\", \"Q2\",4000),\n",
    "    (\"XYZ\", \"Q3\", 1000),\n",
    "    (\"XYZ\", \"Q4\", 2000), \n",
    "    (\"KLM\", \"Q1\", 2000),\n",
    "    (\"KLM\", \"Q2\", 3000),\n",
    "    (\"KLM\", \"Q3\", 1000),\n",
    "    (\"KLM\", \"Q4\", 5000)]\n",
    "\n",
    "column= [\"Company\", \"Quarter\", \"Revenue\"]\n",
    "\n",
    "df= spark.createDataFrame(data=data, schema=column)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b5a9a3-ff91-425b-bb5e-41dd23bacab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Pivot a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3741350c-a775-4ce6-9c68-ef55b8b063f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pivot_df=df.groupBy(\"Company\").pivot(\"Quarter\").sum(\"Revenue\")\n",
    "display(pivot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a09100b-ce32-428a-99a5-532759a41bc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e597b4f-a976-4b93-b2be-14d016a1cdc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_unpiout=pivot_df.selectExpr(\"Company\",\"stact(4,'Q1',Q1,'Q2',Q2,'Q3',Q3,'Q4',Q4) as (Quarter,Revenue)\")\n",
    "display(df_unpiout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62d05675-b9b3-4b4c-ab80-ae7bca97f251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##15. Databricks| Spark | Pyspark | Read Json| Flatten Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f420de9f-9d9f-49fc-ae8c-dddafcb0814d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df=spark.read.option(\"multiline\",\"true\").json(\"file_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8afad64b-b67e-4ab1-8e0c-958b7a06da79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.option(\"multiline\",\"true\").json(\"dbfs:/FileStore/tables/sample4.json\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a74ddee-36ba-4038-a3f9-99797fa4e988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#Flatten array of structs and structs\n",
    "\n",
    "def flatten():\n",
    "\n",
    "#compute Complex Fields (Lists and Structs) in Schema\n",
    "\n",
    "    complex_fields = dict([(field.name, field.dataType)\n",
    "            for field in of.schema.fields\n",
    "            if type(field.dataType)== ArrayType or type(field.dataType)== StructType])\n",
    "while len(complex_fields)!=0:\n",
    "    col_name=list(complex_fields.keys()) [0]\n",
    "    print (\"Processing:\"+col_name+\" Type: \"+str(type (complex_fields[col_name])))\n",
    "\n",
    "#if StructType then convert all sub element to columns.\n",
    "\n",
    "#i.e. flatten structs\n",
    "\n",
    "if (type (complex_fields [col_name]) == StructType):\n",
    "    expanded= [col(col_name+'.'+k).alias (col_name+'.'+k) for k in [n.name for n in complex_fields[col_name]]]\n",
    "    df= df.select(\"*\", *expanded).drop(col_name)\n",
    "#if ArrayType then add the Array Elements as Rows using the explode function\n",
    "#i.e. explode Arrays\n",
    "elif (type(complex_fields[col_name])== ArrayType):\n",
    "   df=df.withColumn(col_name,explode_outer(col_name))\n",
    "    \n",
    "#recompute remaining Complex Fields in Schema\n",
    "\n",
    "complex_fields= dict([(field.name, field.dataType) \n",
    "                      for field in df.schema.fields\n",
    "                      if type(field.dataType)== ArrayType or type(field.dataType) ==StructType])\n",
    "\n",
    "\n",
    "return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c311224d-6fc0-4b85-9049-6f5fb010d6e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flaten_df=flatten(df)\n",
    "display(flaten_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccb85c4d-c151-4549-8a7e-500a23dee432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##16. Databricks | Spark | Pyspark | Bad Records Handling | Permissive;DropMalformed;FailFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5fb6a4d-9ad0-4175-be88-b9b3f443b8c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "df=spark.read.fromat(\"csv\").option(\"mode\",\"dropmalformed\")\n",
    ".option(\"header\",\"true\")\n",
    ".schema(schema)\n",
    ".load(\"path\")###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248a9f13-afb4-4f80-be82-a5326b523cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\").load(\"dbfs:/FileStore/Mahesh/monthlydata2.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d3befba-c1a5-450f-88af-90057107680e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create schema\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "schema=StructType([\n",
    "    StructField(\"Month\",StringType()),\n",
    "    StructField(\" Emp_count\",IntegerType()),\n",
    "    StructField(\" Production_unit\",IntegerType()),\n",
    "    StructField(\" Expense\",IntegerType()),\n",
    "    StructField(\"Corrupt_record\",StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f41b81e8-1400-497a-bee2-dbe34190e3e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Permisive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0b8e49c-23de-49f6-82ee-238d7a61ec34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"mode\",\"PERMISSIVE\").option(\"header\",\"true\").schema(schema).load(\"dbfs:/FileStore/Mahesh/monthlydata2.csv\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "597398ef-1793-42b4-85c7-6cf42bbbf7a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DropMallformed Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa08ef6e-6ff3-4391-bc83-a8aa10809f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"mode\",\"DROPMALFORMED\").option(\"header\",\"true\").schema(schema).load(\"dbfs:/FileStore/Mahesh/monthlydata2.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "250b01b1-637a-4524-83e4-9065b65b170a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##17. Databricks & Pyspark: Azure Data Lake Storage Integration with Databricks\n",
    "1.Using service principle\n",
    "2.Using azure active directory credential\n",
    "3.Udinh ADLS Access key\n",
    "4.Creating mountpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "901b974c-bb35-495c-ab91-42fc87b5997f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This section is not working in community edition we can work with Azureportal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe976215-34c5-477c-86f7-cfad0bec2994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##18. Databricks & Pyspark: Ingest Data from Azure SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82ac0e5a-3898-4904-8af1-9c323d9bbf96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45f3a345-e94b-4bd7-b8c9-702a0c9bdcb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##19. Databricks & Pyspark: Real Time ETL Pipeline Azure SQL to ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f257a464-bd86-4114-ab73-a03b45a965e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92dbac87-e1ee-4c74-8f66-8be1a95a01db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##20. Databricks & Pyspark: Azure Key Vault Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b43c912-0f40-48c7-ad6a-437d9dc1316a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e86acf4-d47d-4572-a6fd-60dd537a2b0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#21. Databricks| Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e3cfd65-4fe1-4848-a500-96857841adaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49f6296b-ca5a-4c77-985a-fa50b5e75c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##22. Databricks| Spark | Performance Optimization | Repartition vs Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc3bfe0-9487-4636-a480-1f05bd768da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "717eb0a3-0186-42ed-a0c7-526921a95983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.files.maxPartitionBytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d023b979-0b56-4ab8-9901-1f14bfc0887d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Generating a data within a spark environment\n",
    "\n",
    "# rdd=sc.parallelize(range(1,11))\n",
    "#rdd.getNumPartitions()\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "df=spark.createDataFrame(range(10),IntegerType())\n",
    "\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2be52f3e-3423-43db-ad14-5b0b8d40c282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# verify the data with in all partitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a408cf76-7f1f-4347-90fe-634e63aa23f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1bfcd7b-5e93-4e28-b664-0b2429d6aad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Read external files in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5efea083-c24a-4747-b089-8b70e397c011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/FileStore/shared_uploads/jbojappa@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c43da1e-1683-46f2-906a-bc113458c7e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"inferSchema\",True).option(\"header\",True).option(\"sep\",\",\").load(\"dbfs:/FileStore/shared_uploads/jbojappa@gmail.com\")\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfecdd27-e824-49fb-8496-6abe7d9e7325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "230ad9a5-b9e1-4e08-83d9-6a7cb18ec364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\",200000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63d062cf-1cf0-4b65-bcad-243a278f2477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"inferSchema\",True).option(\"header\",True).option(\"sep\",\",\").load(\"dbfs:/FileStore/shared_uploads/jbojappa@gmail.com\")\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4733e156-6a46-4348-a6ba-05c858e814a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=df.repartition(20)\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa125c8f-627c-476a-9937-f98545f2a5d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cecf5380-802f-4356-a0de-bb4d845abf0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2=df.coalesce(2)\n",
    "\n",
    "df2.rdd.getNumPartitions()\n",
    "df2.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "070f5971-4b1c-4c38-84bd-205840f0341c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###23. Databricks | Spark | Cache vs Persist | Interview Question | Performance Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6315a475-3762-4d10-9c77-7febbfe48dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##24. Databricks| Spark | Interview Questions| Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "296316a3-03b3-4d14-8c43-265f34321503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#25. Databricks | Spark | Broadcast Variable| Interview Question | Performance Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d046b2c2-27b3-4170-ad31-fb1d15280199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Transaction = [\n",
    "(100, 'Cosmetic', 150),\n",
    " (200, 'Apparel', 250), \n",
    " (300, 'Shirt', 400), \n",
    " (480, 'Trouser', 500), \n",
    " (500, 'Socks', 20), \n",
    " (100, 'Belt', 70), \n",
    " (200, 'Cosmetic', 250),\n",
    "(300, 'Shoe',400),\n",
    "(400, 'Socks', 25),\n",
    "(500, 'Shorts', 100)]\n",
    "\n",
    "transactionDF = spark.createDataFrame (data=Transaction, schema = ['Store_id', 'Item', 'Amount'])\n",
    "\n",
    "transactionDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "774fa293-2567-48ac-9bd8-72dad698e062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Store = [\n",
    "(100, 'Store_London'),\n",
    "(200, 'Store_Paris'),\n",
    "(300, 'Store_Frankfurt'),\n",
    "(400, 'Store_Stockholm'),\n",
    "(500, 'Store_Oslo')]\n",
    "storeDF = spark.createDataFrame (data=Store, schema = ['Store_id','Store_name'])\n",
    "\n",
    "storeDF.show()\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c27d6fac-52b9-4e63-91b0-c6b938a7ece1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "joindf=transactionDF.join(broadcast(storeDF),transactionDF['Store_id']==storeDF['Store_id'])\n",
    "joindf.show()\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4324a594-2f05-4b2f-b909-8a5f4259a66e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joindf.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0efad10-7597-4b91-bf48-41b47d124e2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###26. Databricks | Spark | Adaptive Query Execution| Interview Question | Performance Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a54530b-0588-4d99-a737-5113769224c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.conf.set(\"spark.sql.adaptive.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b890a04-7769-45d0-80c3-eab95f5834db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b91ee309-e539-434e-bbab-fc3181c0efcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##31. Databricks Pyspark: Handling Null - Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e833bde-26b7-4526-872f-687da79568ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Filter all records with Null Value\n",
    "#df.filter(df.Marks.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bad475c-5acc-4f89-86ed-b455b308f66c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_student =[(\"Michael\", \"Science\", 80, \"P\",90), \n",
    "               (\"Nancy\", \"Mathematics\",90,\"P\", None), \n",
    "               (\"David\", \"English\",20,\"F\",80),\n",
    "                (\"John\", \"Science\", None, \"F\", None),\n",
    "                 (\"Blessy\", None, 30, \"F\",50),\n",
    "                  (\"Martin\", \"Mathematics\", None, None, 70)]\n",
    "\n",
    "schema =[\"name\",\"Subject\",\"Mark\", \"Status\", \"Attendance\"]\n",
    "\n",
    "df=spark.createDataFrame(data = data_student, schema = schema)\n",
    "\n",
    "display (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9bef21b-9682-451c-80fe-53117028af77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(df.filter(df.Mark.isNull()))\n",
    "#display(df.filter(\"Mark IS NULL\"))\n",
    "from pyspark.sql.functions import col\n",
    "display(df.filter(col(\"Mark\").isNull()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "333e6378-1479-4aad-ac56-6ce652702109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Filter all records without Null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "869171b7-dc1f-4ff5-ae48-7a84a46e64d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(df.filter(df.Mark.isNotNull()))\n",
    "display(df.filter((df.Mark.isNotNull()) & (df.Attendance.isNotNull()))) # we can use and,or all operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fe71dec-d01b-4797-9d9d-c04310a61943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###32. Databricks| Pyspark| Handling Null Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3457efc5-2308-4dc5-9ac7-29cb23bb6300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create dataframe\n",
    "\n",
    "data_student = [(\"Michael\", \"Science\",80,\"P\",90), \n",
    "                (\"Nancy\", \"Mathematics\",90, \"P\", None), \n",
    "                (\"David\", \"English\",20,\"F\",80), \n",
    "                (\"John\", \"Science\", None, \"F\", None),\n",
    "                 (\"Martin\", \"Mathematics\", None, None, 70),\n",
    "                  (None, None, None, None, None)]\n",
    "\n",
    "Schema = [\"name\",\"Subject\", \"Mark\", \"Status\", \"Attendance\"]\n",
    "\n",
    "df = spark.createDataFrame(data = data_student, schema = Schema)\n",
    "\n",
    "display (df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f769a796-0674-4281-80f3-35de7fef80ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Drop the records with Null value- ALL & ANY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c34abf7f-c93a-411f-8bb4-25e3c11e3570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.na.drop())\n",
    "display(df.dropna(\"any\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2d17b0-dafe-4887-b134-3d8412fdcd75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.dropna(\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3d11b8b-09ee-431e-8238-036081b6c667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Drop the records with Null values on selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8726eb30-bd70-46cd-8510-5516e24f9843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.na.drop(subset=[\"Mark\",\"Attendance\"])) #it could be combination of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "558d5e53-6e34-4844-92ef-8943220abae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "###Fill value for all columns is Null is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41bdfadd-1660-4cb7-88ed-34dc001635b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.na.fill(value=0))\n",
    "display(df.na.fill(value=\"NA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0991bde3-fcb3-4a93-90a1-0478d1b86c95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Fill a value for specific columns if contains null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "400372b2-ac3c-4a1c-b9b8-856cfef6c9a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "display(df.na.fill(value=0,subset=[\"Mark\",\"Attendance\"]))\n",
    "display(df.na.fill({\"Mark\":0,\"Status\":\"NA\",\"name\":\"No_name\",\"Subject\":\"English\",\"Attendance\":50}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5bd97f6-92f6-44ae-b24f-c507f5baea7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##33. Databricks | Spark | Pyspark | UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f9fa28-a7d5-4ccb-b168-4eebfb035225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_data = [\n",
    "    (10, \"Michael Robinson\", \"1999-06-01\",\"100\", 2000),\n",
    "     (20, \"James Wood\", \"2003-03-01\",\"200\",8000), \n",
    "     (30, \"Chris Andrews\", \"2005-04-01\",\"100\",6000),\n",
    "(40, \"Mark Bond\", \"2008-10-01\",\"100\", 7000),\n",
    "(50, \"Steve Watson\", \"1996-02-01\",\"400\",1000),\n",
    "(60, \"Mathews Simon\", \"1998-11-01\",\"500\",5000),\n",
    "(70, \"Peter Paul\", \"2011-04-01\",\"600\",5000)]\n",
    "\n",
    "employee_schema = [\"employee_id\", \"Name\", \"doj\", \"employee_dept_id\", \"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame (data=employee_data, schema = employee_schema)\n",
    "\n",
    "display (empDF)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "540ff0a6-cfd2-44b2-b2f7-8b4dcb97b73d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define  UDF to Rename column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52ece8ab-01d7-47af-a1e1-812dac3a6a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "def rename_columns(rename_df):\n",
    "    for column in rename_df.columns:\n",
    "        new_column = \"Col\"+column\n",
    "        rename_df = rename_df.withColumnRenamed(column,new_column)\n",
    "    return rename_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5bcc93-6197-46c4-8e9a-59fd88804e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rename_df = rename_columns(empDF)\n",
    "display(rename_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b65c961-db8e-492d-8056-323f8849d583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "def rename_columns(rename_df):\n",
    "    for column in rename_df.columns:\n",
    "        new_column = \"UDF \"+column\n",
    "        rename_df = rename_df.withColumnRenamed(column,new_column)\n",
    "    return rename_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "774153f2-9072-4978-bdce-e9e78698621a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rename_df = rename_columns(empDF)\n",
    "display(rename_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "302ecb49-4ea2-4fd1-a1a2-a5a3837df4e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##UDF To convert name into upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6720d857-33b3-4824-b77c-9518bab7c738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,upper\n",
    "\n",
    "def upperCase_col(df):\n",
    "  empDF_upper=df.withColumn(\"name_upper\",upper(df.Name))\n",
    "  return empDF_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec90afb2-1a17-40b9-b2d0-1872ab6117dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "up_case_df=upperCase_col(empDF)\n",
    "display(up_case_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6786f29a-ae6e-42e6-bc8c-32d26df8e484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4fa7441-820a-4da9-93ed-1f90e5a61cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###34. Databricks - Spark: Data Skew Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15e0c0cb-e82a-470b-a4de-230684d4b5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###35. Databricks & Spark: Interview Question - Shuffle Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dbb3e70-2c8a-4eea-8d63-9dc2ea465a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###36. Databricks: Autoscaling | Optimized Autoscaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccd3b364-8fd0-4abe-8a6a-c83f11ea6274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###37. Databricks | Pyspark: Dataframe Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e341d456-7f19-4e7b-b39d-73c7b142b0bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5eca8eb-8233-458c-9e56-54df78246023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###38. Databricks | Pyspark | Interview Question | Compression Methods: Snappy vs Gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40cd3a31-f08e-4445-a4a4-f8c0f812af43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csvfile = \"dbfs:/FileStore/shared_uploads/jbojappa@gmail.com/EMPLOYEE.csv\"\n",
    "\n",
    "df = (spark.read\n",
    "    .option('sep',\",\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .csv(csvfile)\n",
    "    )\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf7e3f24-5e1f-48a4-8b54-f975e4bafbb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").option(\"compression\",\"snappy\").save(\"dbfs:/FileStore/shared_uploads/jbojappa@gmail.com/snappy_parquets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c36eb5-8a84-44f5-b02d-206016c5fc43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/FileStore/shared_uploads/jbojappa@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4848f9-9697-4392-b5d4-9b3473bf1330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").option(\"compression\",\"gzip\").save(\"dbfs:/FileStore/shared_uploads/jbojappa@gmail.com/gzip_parquets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e57c2e2a-5db4-4913-a7c0-ba86e5ae6966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###39. Databricks | Spark | Pyspark Functions| Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e27b2f2-3dfe-489a-b484-0a04c63ef553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_data = [\n",
    "    (10, \"Michael Robinson\", \"1999-06-01\",\"100\", 2000),\n",
    "     (20, \"James Wood\", \"2003-03-01\",\"200\",8000), \n",
    "     (30, \"Chris Andrews\", \"2005-04-01\",\"100\",6000),\n",
    "(40, \"Mark Bond\", \"2008-10-01\",\"100\", 7000),\n",
    "(50, \"Steve Watson\", \"1996-02-01\",\"400\",1000),\n",
    "(60, \"Mathews Simon\", \"1998-11-01\",\"500\",5000),\n",
    "(70, \"Peter Paul\", \"2011-04-01\",\"600\",5000)]\n",
    "\n",
    "employee_schema = [\"employee_id\", \"Name\", \"doj\", \"employee_dept_id\", \"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame (data=employee_data, schema = employee_schema)\n",
    "\n",
    "display (empDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f29ea6-963c-4dcb-8a79-fdd7d0ea9bf5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "First Method"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split,col\n",
    "df = empDF.withColumn('first_name', split(empDF['Name'], ' ').getItem(0)) \\\n",
    "       .withColumn('last_name', split(empDF['Name'], ' ').getItem(1))\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79c1f77d-2271-4f35-a169-de5a9120682d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Second Method"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import split,col\n",
    "\n",
    "split_col = split(empDF['Name'], ' ')\n",
    "# Create new columns from the split data\n",
    "df = empDF.withColumn('first_name', split_col.getItem(0)) \\\n",
    "       .withColumn('last_name', split_col.getItem(1)) \n",
    "       \n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d24b722-4b46-423b-8e53-cfc6d2b17dee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Third Methos"
    }
   },
   "outputs": [],
   "source": [
    "split_col=split(empDF['doj'],'-')\n",
    "df3=empDF.select('employee_id','Name','employee_dept_id','salary',split_col.getItem(0).alias('joining_year'),split_col.getItem(1).alias('joining_month'),split_col.getItem(2).alias('joining_day'))\n",
    "df3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5bca08-8d33-4da3-be0c-ce7539743612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"John,Doe,30\",), (\"Jane,Smith,25\",)]\n",
    "columns = [\"info\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Correctly split the 'info' column by comma\n",
    "split_col = split(df['info'], ',')  # 'split' is correctly used here\n",
    "\n",
    "# Create new columns from the split data\n",
    "df = df.withColumn('first_name', split_col.getItem(0)) \\\n",
    "       .withColumn('last_name', split_col.getItem(1)) \\\n",
    "       .withColumn('age', split_col.getItem(2))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99468c13-9f55-49e4-b700-802512df4fe9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split and Drop splitedcolumns"
    }
   },
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"John,Doe,30\",), (\"Jane,Smith,25\",)]\n",
    "columns = [\"info\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Correctly split the 'info' column by comma\n",
    "split_col = split(df['info'], ',')  # 'split' is correctly used here\n",
    "\n",
    "# Create new columns from the split data\n",
    "df = df.withColumn('first_name', split_col.getItem(0)) \\\n",
    "       .withColumn('last_name', split_col.getItem(1)) \\\n",
    "       .withColumn('age', split_col.getItem(2))\\\n",
    "       .drop(df['info'])\n",
    "      \n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6c07d26-3e1e-4a69-ab71-82722e1f16f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dac635ff-9996-4b79-a146-24b3c830ed01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###40. Databricks | Spark | Pyspark Functions| Arrays_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c16c79-d09b-496a-ab71-39b1a2c814d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "40. Databricks | Spark | Pyspark Functions| Arrays_zip"
    }
   },
   "outputs": [],
   "source": [
    "aray_data=[(\"john\",4, 1),\n",
    "           (\"john\",4, 2),\n",
    "           (\"john\",5, 3),\n",
    "           (\"clerk\",6, 6),\n",
    "           (\"clerk\",4, 4),\n",
    "           (\"clerk\",8, 1),\n",
    "           (\"john\",4, 9),\n",
    "           (\"clerk\",7, 1),\n",
    "           (\"clerk\",4, 7),\n",
    "           (\"clerk\",2, 1),\n",
    "           (\"john\",4, 3),\n",
    "           (\"john\",5, 1)]\n",
    "\n",
    "array_schema=['name','score1','scoe2']\n",
    "\n",
    "arraydf=spark.createDataFrame(aray_data,array_schema)\n",
    "arraydf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd67952d-f348-44ac-a1fd-97f6021854fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "masterDF = arraydf.groupby(\"name\").agg(F.collect_list('score1').alias('Array_Score_1'),F.collect_list('scoe2').alias('Array_Score_3'))\n",
    "masterDF.display()\n",
    "masterDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ec9f94-5a77-4617-a54d-654ae6f0c488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as arrays_zip\n",
    "array_zipdf = masterDF.withColumn(\"Zipped_volume\", F.arrays_zip(\"aray_scoe1\",\"array_score2\"))\n",
    "array_zipdf.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aeb3dea-ecd7-41f5-b4ec-8ba2babd2892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##42. Databricks | Spark | Pyspark Functions| Part 3 : Array_Except"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d225d78-6c03-4ba6-92e4-87d620e692c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##43. Databricks | Spark | Pyspark Functions| Part 4 : Array_Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b9c90a-4072-406c-8531-41f523770890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "empdf=[(\"John\",[4,6,7,9,2]),\n",
    "       (\"David\",[7,5,1,4,7,1]),\n",
    "       (\"Mike\",[3,9,1,6,2])\n",
    "       ]\n",
    "\n",
    "schema=['name','array_col']\n",
    "df=spark.createDataFrame(empdf,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc78b2f-804c-435d-a42d-dc80b2b1f1d5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Applay array intersect"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_sort=df.withColumn(\"sorted\",F.array_sort(df[\"array_col\"]))\n",
    "df_sort.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "180dadeb-84c9-4ec7-a771-bd756073cddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##44. Databricks | Spark | Python Functions| Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c7b8582-e0c3-4e6f-9f1a-312ab72fad98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##45. Databricks | Spark | Pyspark | PartitionBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3745d25-bbe7-4f7d-af17-783c5728121a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df.write.partitionBy(key).csv(path)\n",
    "#df.writie.option(\"header\",\"True\").partitionBy(\"Year\").mode(\"overwrite\").csv(\"filepath\")\n",
    "#df.writie.option(\"header\",\"True\").partitionBy(\"Year\",\"sex\").mode(\"overwrite\").csv(\"filepath\")\n",
    "#df.writie.option(\"header\",\"True\").option(\"maxRecordsPerFile\",4200).partitionBy(\"Year\").mode(\"overwrite\").csv(\"filepath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a3f1ee-5447-446a-8f40-d0bbdc7726a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Upload fils list\n",
    "dbutils.fs.ls(\"dbfs:/FileStore/boja1.csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4988e57-cdc7-492d-afc4-c00c25ad311c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "raw_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"InferSchema\",\"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .load(\"dbfs:/FileStore/boja1.csv/baby_names.csv\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d49ed5a6-b7b4-4dce-a51c-b21e7b94d2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26e9f9fe-8916-418a-a61f-2b1f0314ee27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"sex\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"dbfs:/BOJA/FILS/babyoutpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e94abef-404e-4d62-af86-cc5a9a54a226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/BOJA/FILS/babyoutpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce34c2ca-a91d-409b-b348-99c62a3939fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(raw_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4a0a58e-c258-4971-8b3a-3e06a33014af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b26e8f5-2c3a-4358-bce6-2a52802acfad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.display()\n",
    "total_count=raw_df.count()\n",
    "print(f\"Total count: {total_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3243bf9c-5cc6-4593-bc3a-953ade646732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "159d6688-0af9-42d7-af0b-5147e1ac4205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(raw_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e2698d4-3df8-41b5-ab4f-5543c3ecd537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Number of records per partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c22c6320-0c97-4af3-9043-df5f1b109263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "raw_df.withColumn(\"partitionID\",spark_partition_id()).groupBy(\"partitionID\").count().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402a0ceb-88c6-49ea-9b96-e3c9abfe6273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dae0d15-30a8-40ef-a052-9744b68925db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Repartition the dataframe to 5"
    }
   },
   "outputs": [],
   "source": [
    "df5=raw_df.select(raw_df.year,raw_df.count,raw_df.sex,raw_df.count).repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "927e4d0c-ffa3-411c-9e6d-2c557f821a08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df5 = raw_df.select(raw_df.year, raw_df.count, raw_df.sex).repartition(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a327c17-658c-4ceb-a320-c4ed8f0f0586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##47. Databricks | Spark | Pyspark | Null Count of Each Column in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "172f9ee1-a97b-40a8-aa93-de580054027c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_student=[(\"michale\",\"scienct\",89,\"p\",90),\n",
    "              (\"nacy\",\"english\",80,\"p\",None),\n",
    "              (\"david\",\"social\",60,\"p\",80),\n",
    "              (\"john\",\"maths\",70,\"f\",60),\n",
    "              (\"martin\",\"physics\",None,\"f\",None),\n",
    "              (\"michale\",\"chemistry\",None,None,50),\n",
    "              (None,None,None,None,None)]\n",
    "\n",
    "schema=[\"name\",\"subject\",\"marks\",\"statuss\",\"attendance\"]\n",
    "sample_df=spark.createDataFrame(data_student,schema)\n",
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ba95cd1-fb5d-4ca9-90bb-dc58c80b2a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Find the count of Null occurences of Each Columns in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5565c00b-c8f7-443d-9524-7553bda42eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,count,when\n",
    "result=sample_df.select([count(when(col(c).isNull(), c)).alias(c) for c in sample_df.columns])\n",
    "result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abbea12b-8ae8-4284-bc11-409f764b4f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##48. Databricks - Pyspark: Find Top or Bottom N Rows per Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0501cfd0-210d-4970-8e48-810d4dc534ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_student=[(\"michale\",\"scienct\",89,\"p\",90),\n",
    "              (\"david\",\"english\",80,\"p\",100),\n",
    "              (\"david\",\"social\",60,\"p\",80),\n",
    "              (\"david\",\"maths\",70,\"f\",60),\n",
    "              (\"david\",\"physics\",50,\"f\",70),\n",
    "              (\"michale\",\"chemistry\",40,\"p\",70),\n",
    "              (\"michale\",\"chemistry\",60,\"p\",60),\n",
    "              (\"michale\",\"chemistry\",70,\"p\",80),\n",
    "              (\"michale\",\"chemistry\",80,\"p\",90),\n",
    "              (\"arjun\",\"chemistry\",90,\"p\",70),\n",
    "              (\"arjun\",\"chemistry\",45,\"p\",50),\n",
    "              (\"arjun\",\"chemistry\",70,\"p\",40),\n",
    "              (\"arjun\",\"alib\",50,\"f\",80)]\n",
    "\n",
    "schema=[\"name\",\"subject\",\"marks\",\"statuss\",\"attendance\"]\n",
    "df=spark.createDataFrame(data_student,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331bf9d5-c9fc-480f-8465-e4bc09740eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a rank with in each  group Name\n",
    "\n",
    "from pyspark.sql.window import Window \n",
    "from pyspark.sql.functions import col, row_number\n",
    "windowdept=Window.partitionBy(\"name\").orderBy(col(\"marks\").desc())\n",
    "df2=df.withColumn(\"row\",row_number().over(windowdept)).orderBy(\"name\",\"row\")\n",
    "df2.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf0d1d4-25f8-46f5-b2cd-68399db33eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get top n row per grou name\n",
    "df3=df2.filter(col(\"row\")<=2) #<=1\n",
    "df3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ffcabe2-bc0b-4245-8de1-6c59c346a7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a rank with in each grop of subjecgt\n",
    "windodept=Window.partitionBy(\"subject\").orderBy(col(\"marks\"))\n",
    "df6=df.withColumn(\"row\",row_number().over(windodept)).orderBy(\"name\",\"row\")\n",
    "df6.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "926f8ad9-7202-4a8f-8dda-9d530f8695c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get top n row per group\n",
    "df7=df6.filter(col(\"row\")<=1) #<=1\n",
    "df7.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7b2adaf-dc53-4017-a118-6a2dbfc5edbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##49. Databricks & Spark: Interview Question(Scenario Based) - How many spark jobs get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e56cab1-03c1-4b09-b86b-e147c3b3af56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read csv file without option(ONLY ONE JOB CREATED)\n",
    "raw_df=spark.read.format(\"csv\").load(\"dbfs:/BOJA/FILS/babyoutpu\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef5f03c-fb37-490a-ae48-7f5a1eaf610e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read csv file with inferschema(TWO JOBS CREATED)\n",
    "raw_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"InferSchema\",\"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .load(\"dbfs:/FileStore/boja1.csv/baby_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cc9d1f1-d6f6-49bd-8971-fba8f37eaaa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read csv file with define schema(No jobs created)\n",
    "from pyspark.sql.types import StructType, StructField,StringType,IntegerType\n",
    "schema=StructType([StructField(\"year\",IntegerType(),False), \\\n",
    "    StructField(\"name\",StringType(),False), \\\n",
    "    StructField(\"percent\",IntegerType(),False), \\\n",
    "    StructField(\"sex\",StringType(),False)\n",
    "    ])\n",
    "\n",
    "defined_schema = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"dbfs:/FileStore/boja1.csv/baby_names.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca0b12d6-fc20-4ee8-92fd-65283e90cc24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##50. Databricks | Pyspark: Greatest vs Least vs Max vs Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c223083-170a-4d09-bb0c-7d9078f75f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_data=[(\"david\",80,70,90,60,65),\n",
    "            (\"Kevin\",30,70,60,20,65),\n",
    "            (\"Raju\",60,70,20,60,65),\n",
    "            (\"Lakshmi\",80,70,90,10,65),]\n",
    "schema=[\"Name\",\"subject1\",\"subject2\",\"subject3\",\"subject4\",\"subject5\"]\n",
    "df=spark.createDataFrame(input_data,schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46f1365-30b8-4b59-8e32-1f828ac7ad71",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Greatest of Columns"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import greatest\n",
    "greatestdf=df.withColumn(\"Greatest\",greatest(\"subject1\",\"subject2\",\"subject3\",\"subject4\",\"subject5\"))\n",
    "display(greatestdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa7aec6-b21c-4aa3-b3fc-04a53f49a67e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Least of Columns"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import least\n",
    "leastdf=df.withColumn(\"Least\",least(\"subject1\",\"subject2\",\"subject3\",\"subject4\",\"subject5\"))\n",
    "leastdf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b876c3ca-e73d-4817-b4cf-a662a0b2b8fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Max of rows"
    }
   },
   "outputs": [],
   "source": [
    "df.agg({\"subject1\":'max'}).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd2d878-f0d0-4793-9eb6-0498d5b061cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.agg({\"subject1\":'max',\"subject2\":'max',\"subject3\":'max',\"subject4\":'max',\"subject5\":'max'}).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4dbe851-b43d-4309-9af3-205de91abe6a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Min of rows"
    }
   },
   "outputs": [],
   "source": [
    "df.agg({\"subject1\":'min',\"subject2\":'min',\"subject3\":'min',\"subject4\":'min',\"subject5\":'min'}).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfb8bd44-9c0e-46ed-a64f-b000e64e729c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##51. Databricks | Pyspark | Delta Lake: Introduction to Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50e9d2cf-3997-4a46-9bca-b69e66e059e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##52. Databricks| Pyspark| Delta Lake Architecture: Internal Working Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c3211b-dd1c-4947-99b3-1e36c567e78f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create delta table"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from delta.tables  import*\n",
    "\n",
    "DeltaTable.create(spark) \\\n",
    "    .tableName(\"deltainternal_demo\") \\\n",
    "    .addColumn(\"emp_id\",\"int\") \\\n",
    "    .addColumn(\"emp_name\",\"string\") \\\n",
    "    .addColumn(\"gender\",\"string\") \\\n",
    "    .addColumn(\"salary\",\"int\") \\\n",
    "    .addColumn(\"edept\",\"int\") \\\n",
    "    .property(\"description\",\"table created for demo purpose\") \\\n",
    "    .location()\n",
    "    .exucute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b01dc8ad-369a-468e-b680-b4a9ecaf53c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##54. Databricks | Delta Lake| Pyspark: Create Delta Table Using Various Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db710a20-b156-4b59-a8d6-73745b0ea402",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Method 1 Pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables  import*\n",
    "\n",
    "DeltaTable.create(spark) \\\n",
    "    .tableName(\"employee_demo\") \\\n",
    "    .addColumn(\"emp_id\",\"int\") \\\n",
    "    .addColumn(\"emp_name\",\"string\") \\\n",
    "    .addColumn(\"gender\",\"string\") \\\n",
    "    .addColumn(\"salary\",\"int\") \\\n",
    "    .addColumn(\"edept\",\"int\") \\\n",
    "    .property(\"description\",\"table created for demo purpose\") \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7f0d37a-5d24-4aa6-8844-616cc4e196c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##55. Databricks| Pyspark| Delta Lake: Delta Table Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4978dab1-6bab-4269-b78c-85863c5e743e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables  import*\n",
    "DeltaTable.create(spark) \\\n",
    ".tableName(\"employee_demo\") \\\n",
    ".addColumn(\"emp_id\",\"int\") \\\n",
    ".addColumn(\"emp_name\",\"string\") \\\n",
    ".addColumn(\"gender\",\"string\") \\\n",
    ".addColumn(\"salary\",\"int\") \\\n",
    ".addColumn(\"edept\",\"int\") \\\n",
    ".property(\"description\",\"table created for demo purpose\") \\\n",
    ".location(\"tales/deltaTable/_delta_log/createtable\") \\\n",
    " .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4694d207-f2fb-4750-8443-770eb2b4695e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f071fa6-ff86-4ad5-88c7-0a29d21b28e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##57. Databricks| Pyspark| Delta Lake: Different Approaches to Delete Data from Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8670aed6-b8b2-4033-949c-8c7e97030234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables  import*\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    ".tableName(\"employee_demo\") \\\n",
    ".addColumn(\"emp_id\",\"int\") \\\n",
    ".addColumn(\"emp_name\",\"string\") \\\n",
    ".addColumn(\"gender\",\"string\") \\\n",
    ".addColumn(\"salary\",\"int\") \\\n",
    ".addColumn(\"edept\",\"int\") \\\n",
    ".property(\"description\",\"table created for demo purpose\") \\\n",
    ".location(\"/FileStore/tables/deltaTable/_delta_log/path_employee_demo\") \\\n",
    " .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0fb2b35-36d5-4111-8c39-f92334434e6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Populated sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd60f6e-cb34-415c-86ee-5a0fd2cf8f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "insert into employee_demo values(100,\"stephn\",\"M\",2000,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79853f0-fd53-4829-90ef-472d17fc37ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from employee_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "902e2191-4688-4b32-a5d5-8730cec2c55c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DELETE FROM delta./FileStore/tables/deltaTable/_delta_log/path_employee_demo where emp_id=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c4d1bf-a2e8-472e-909c-ce73fe2e103f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "dir(pyspark.sql.dataframe.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5be63bcb-c0e0-4f0b-9581-b3bd1d5d9959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##58. Databricks | Pyspark | Delta Lake : Update Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c43cec-b32f-4fdc-956d-35af347dfaf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table boja_demo (\n",
    "  emp_id int,\n",
    "emp_name string,\n",
    "gender string,\n",
    "salary int,\n",
    "edept int\n",
    ")\n",
    "using delta\n",
    "location '/FileStore/tables/deltaTable/_delta_log/path_boja_demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcd90d41-bf72-45f6-aa9a-4f46cd98e4c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "insert into boja_demo values(100,\"bojappa\",\"M\",500,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad6b17c1-d251-48c9-93c9-530b16bf1bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM BOJA_DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40f34397-cdc1-408d-b4d8-82a341441ac6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dataframe creation"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create data\n",
    "data = [\n",
    "    (1, \"Alice\", 30),\n",
    "    (2, \"Bob\", 25),\n",
    "    (3, \"Charlie\", 35)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Display DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79690f2d-9ab1-4565-8cd6-29c305c1d0b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "1.06 STARTING VEDIOS"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1022354636463027,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Rajadataengineer_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
